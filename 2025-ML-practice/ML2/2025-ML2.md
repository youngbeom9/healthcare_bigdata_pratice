## 1.  회귀 분석(Regression) 알고리즘 구현하기

이번 실습에서는 간단한 파이썬 코드 작성을 통해 ***회귀 분석 알고리즘을 구현***해 보도록 하겠습니다.

이론 강의를 통해 회귀 모델은 다음과 같이 표현한다고 하였습니다.

Y=β0+β1X

여기서 적절한 β0 와 β1 를 찾기 위해서는 데이터의 실제 값과 모델이 예측하는 값의 차이를 ***최소한으로 하는 선***을 찾아야 합니다.

차이를 최소한으로 하는 선을 찾는 회귀 분석의 절차는 다음과 같습니다.



------

***회귀 분석 절차***

1. X라는 값이 입력되면, Y=β0+β1X라는 계산식을 통해 값을 산출하는 ***예측 함수를 정의***합니다.
2. ***실제 값 y와 예측 함수로부터의 예측값 pred_y간의 차이를 계산***합니다.
3. 계산한 차이에 기반하여 ***β0와 β1를 업데이트하는 규칙을 정의***하고 이를 바탕으로 ***β0와 β1의 값을 조정***합니다.
4. 위의 과정을 특정 ***반복 횟수(iteration) 만큼 반복***합니다.
5. 반복적으로 수정된 β0와 β1를 바탕으로 ***Y=β0+β1X라는 회귀식을 정의***합니다.

## ***실습***

1. `beta_0,` `beta_1`, `X` 를 받아 회귀식을 통해 예측값 `y_pred`를 계산하여 반환하는 `prediction()` 함수를 구현합니다.
2. 반복 횟수만큼 ***오차(loss)***를 계산하고 `beta_0`,`beta_1`의 값을 변경하는 함수인 `gradient_descent()` 를 구현합니다.
3. - 앞서 구현한 `prediction()`, `update_beta()`를 활용하세요.
   - 실제 값 y와 prediction 함수를 통해 예측한 예측값 pred_y 간의 차이(loss)를 계산합니다.
   - `loss`는 ***실제값(y) - 예측값(pred_y)***으로 정의합니다.
   - 구현된 함수를 이용하여 `beta_0`와 `beta_1` 의 변화값을 각각 `beta0_delta`, `beta1_delta`에 저장합니다.
4. 실행 버튼을 눌러 회귀 분석 과정의 시각화를 확인합니다.
5. 제출 버튼을 눌러 올바른 회귀 알고리즘 구현 여부를 확인합니다.



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## 2. 단순 선형 회귀 모델 구현하기

입력값 X가 ***1개***인 경우 적용하는 회귀 알고리즘인 ***단순 선형 회귀 모델***에 대해 알아보았습니다.

실습 1에서와 같이 회귀 모델의 함수식 Y=β0+β1X을 이용하여 알고리즘을 직접 구현할 수도 있지만, ***사이킷런(scikit-learn)*** 이라고 하는 머신러닝 라이브러리에 구현되어 있는 모델을 불러오는 것만으로도 쉽게 단순 선형 회귀 알고리즘을 사용할 수 있습니다.

사이킷런은 머신러닝 모델 구현뿐만 아니라 예시 데이터셋, 데이터 전처리, 세부 조정, 모델 평가 등과 같은 유용한 기능들을 제공합니다.

이번 실습에서는 사이킷런을 활용하여 단순 선형 회귀를 구현하는 방법을 익혀보도록 하겠습니다.



------



***데이터 준비를 위한 사이킷런 함수/라이브러리***

- `from sklearn.model_selection import train_test_split` : 학습용 데이터와 테스트용 데이터를 나누어주는 함수(`train_test_split()`)를 불러옵니다.



------



***단순 선형 회귀를 위한 사이킷런 함수/라이브러리***

- `from sklearn.linear_model import LinearRegression`: 단순 선형 회귀 모델을 불러옵니다.
- `model=LinearRegression()` : 선형회귀 모델 `model`을 정의합니다.
- `model.fit(X, y)`: 인공지능 모델 `model`을 X, y 데이터셋에 대해서 학습시킵니다.
- `model.predict(X)`: 모델 `model`의 X 데이터에 대한 예측값을 반환합니다.
- `model.score(X, y)` : 테스트 데이터를 인자로 받아 학습이 완료된 모델 `model`의 평가 점수를 출력합니다.
- `model.intercept_` : 학습이 완료된 모델 `model`의 β0 를 반환합니다.
- `model.coef_` : 학습이 완료된 모델 `model`의 β1 를 반환합니다.

## ***실습***

1. 데이터를 생성하고, 생성한 데이터를 학습용, 테스트용 데이터로 분리하여 반환하는`load_data()` 함수를 구현합니다.
2. - ***학습용 데이터로 전체 데이터의 70%***를 사용하고,***테스트용 데이터로 나머지 30%***를 사용합니다.
   - 동일한 결과 확인을 위하여 **`*random_state*`**를 ***0***으로 설정합니다.
3. 단순 선형 회귀 모델을 불러와 학습을 진행하고, 해당 모델을 반환하는 함수`regression_model()` 함수를 구현합니다.
4. - 사이킷런에 구현되어 있는 ***단순 선형회귀 모델***을 불러옵니다.
   - 불러온 모델을 학습용 데이터에 맞춰 학습시킵니다.
5. 모델 학습 및 예측 결과 확인을 위한 `main()` 함수를 완성합니다.
6. - 학습이 완료된 모델을 활용하여 테스트 데이터에 대한 예측을 수행합니다.
   - 사이킷런 회귀 모델 내에 구현되어 있는 `score()` 함수를 사용하여 모델 학습 평가 점수를 `model_score` 변수에 저장합니다.
   - `score()`에는 ***테스트용 데이터 X와 y 값***을 각각 넣어야 합니다.
   - 학습된 모델의 beta_0와 beta_1을각각 변수 `beta_0`와 `beta_1`에 저장합니다.
7. 실행 버튼을 눌러 모델이 학습되고 난 이후의 그래프 결과와 모델 점수를 확인합니다.

- - 아직 회귀 알고리즘의 평가 지표에 대해 학습하지 않았지만, 결과 모델 평가 점수가 높을수록 예측이 잘 된 모델이라고 할 수 있습니다.

[실행 결과]

![image_output (4).png](https://cdn-api.elice.io/api-attachment/attachment/c85eee129d9d4232800cd1eb841b6ea3/image_output%20%284%29.png)



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## 3. 다중 선형 회귀 모델 구현하기

다중 선형 회귀는 입력값이 1개일 경우 적용하는 단순 선형 회귀 알고리즘과 달리 ***입력값*** X***가 여러 개일 때*** 사용할 수 있는 회귀 알고리즘입니다.

다중 선형 회귀 또한 사이킷런에 구현되어 있는 라이브러리를 활용하여 간단하게 모델을 구현해보겠습니다.

단순 선형 회귀 실습을 통해 `LinearRegression` 클래스를 정의하고 사용한 것을 기억하시나요?

사실 위 클래스는 다중 선형 회귀에서도 사용이 가능합니다. 사이킷런에서는 ***선형 회귀 라는 이름으로 단순/다중 선형 회귀의 구분 없이 동일한 모듈을 활용***할 수 있습니다.

사이킷런에 저장된 데이터를 불러오고, 불러온 데이터를 다중 선형 회귀 모델을 사용해 예측을 진행해보도록 하겠습니다.

우리가 사용할 데이터는 ***1978년에 발표된 '보스턴 주택 가격 데이터'***로, 미국 보스턴 지역의 주택 가격에 영향을 미치는 요소들(Xi) 및 주택 가격(y)으로 구성되어 있습니다.



------

***다중 선형 회귀를 위한 사이킷런 함수/라이브러리***

- `from sklearn.linear_model import LinearRegression`: 다중 선형 회귀 모델을 불러옵니다.
- `model=LinearRegression()` : 다중선형회귀 모델 `model`을 정의합니다.
- `model.fit(X, y)`: 인공지능 모델 `model`을 X, y 데이터셋에 대해서 학습시킵니다.
- `model.predict(X)`: 모델 `model`의 X 데이터에 대한 예측값을 반환합니다.
- `model.score(X, y)` : 테스트 데이터를 인자로 받아 학습이 완료된 모델 `model`의 평가 점수를 출력합니다.
- `model.intercept_` : 학습이 완료된 모델 `model`의 β0 를 반환합니다.
- `model.coef_` : 학습이 완료된 모델 `model`의 β1 를 반환합니다.

## ***실습***

1. 사이킷런에 존재하는 데이터를 불러오고, 불러온 데이터를 학습용, 테스트용 데이터로 분리하여 반환하는 `load_data()` 함수를 구현합니다.
2. - 사이킷런에 존재하는 boston 데이터를(X, y)의 형태로 불러옵니다.
   - 불러온 데이터를 학습용 데이터와 테스트용 데이터로 분리합니다.
   - ***학습용 데이터로 전체 데이터의 80%***를 사용하고, ***테스트용 데이터로 나머지 20%***를 사용합니다. 동일한 결과 확인을 위하여`random_state`를 ***100***으로 설정합니다.
3. 사이킷런에 구현되어 있는 다중 선형회귀 모델을 불러와 학습용 데이터에 대한 학습을 진행하고, 해당 모델을 반환하는 함수 `Multi_Regression()` 을 구현합니다.
4. - 사이킷런에 구현되어 있는 다중 선형회귀 모델을 불러옵니다.
   - 불러온 모델을 학습용 데이터에 맞춰 학습시킵니다.
5. 모델 학습 및 예측 결과 확인을 위한 `main()` 함수를 완성합니다.
6. - 학습이 완료된 모델을 활용하여 테스트 데이터에 대한 예측을 수행합니다.
   - 사이킷런 회귀 모델 내에 구현되어 있는 `score()` 함수를 사용하여 모델 학습 평가 점수를 `model_score` 변수에 저장합니다.
   - `score()`에는 ***테스트용 데이터 X와 y 값***을 각각 넣어야 합니다.
   - 학습된 모델의 beta_0와 beta_i 들을 각각 변수 `beta_0`와 `beta_i_list`에 저장합니다.
7. 실행 버튼을 눌러 최적의 β0와 각 입력값 Xi들에 대한 βi 값을 확인합니다.



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## 4. 다항 회귀 모델 구현하기

다항 회귀는 Y를 X에 대한 임의의 다항 함수로 모델링하는 선형 회귀를 의미합니다.

다항 회귀는 먼저 입력 데이터 X에 대한 전처리를 진행해준 후 다중 선형 회귀를 적용함으로써 구현됩니다.

사이킷런을 이용하면 입력 데이터에 대한 ***변환(전처리)*** 을 편리하게 진행할 수 있습니다.

따라서 이번 시간에는 사이킷런을 활용하여 다항 회귀를 구현해보겠습니다.



------

***다항 회귀를 위한 사이킷런 함수/라이브러리***

- `poly=PolynomialFeatures(degree, include_bias)`: Polynomial 객체 `poly`를 생성합니다.
- - `degree`: 만들어줄 다항식의 차수를 의미합니다.
  - `include_bias`: 편향 변수의 추가 여부를 설정합니다.(True/False) True로 설정하게 되면, 해당 다항식의 모든 거듭제곱이 0일 경우 편향 변수를 추가합니다. 이는 회귀식에서 β0 와 같은 역할을 합니다.
- `poly.fit_transform(X)`: 데이터 X와 X의 degree 제곱을 추가한 데이터를 반환합니다.
- `from sklearn.linear_model import LinearRegression`: 다중 선형 회귀 모델을 불러옵니다.
- `model=LinearRegression()` : 다중선형회귀 모델 `model`을 정의합니다.
- `model.fit(X, y)`: 인공지능 모델 `model`을 X, y 데이터셋에 대해서 학습시킵니다.

## ***실습***

1. PolynomialFeature 객체를 활용하여 각 변수 값을 제곱하고, 데이터에 추가하는 함수 `Polynomial_transform()`를 구현합니다.
2. - Polynomial 객체를 생성합니다. `degree`를 ***2***로 설정하고, `include_bias`파라미터를 `True`로 설정합니다.
   - 생성한 PolynominalFeatures 객체로 입력 데이터 **`*X*`*****를*** **`*X*`*****의*** **`*degree*`*****제곱 형태***로 바꿔,`poly_X`에 저장합니다.
3. 사이킷런에 구현되어 있는 다중 선형회귀 모델을 불러와 전체 데이터에 대한 학습을 진행하고, 해당 모델을 반환하는 함수 `Multi_Regression()`을 구현합니다.
4. - 사이킷런에 구현되어 있는 다중 선형회귀 모델을 불러옵니다.
   - 불러온 모델을 제곱값이 추가된 데이터에 맞춰 학습시킵니다.
5. 모델 학습 및 예측 결과 확인을 위한`main()` 함수를 완성합니다.

[실행 결과]

![image_output (5).png](https://cdn-api.elice.io/api-attachment/attachment/d9792bac0fca4a9d8c692eeb55659d05/image_output%20%285%29.png)



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## 5. 과적합 방지 기법 - 교차 검증

과적합을 방지하기 위해서 어떠한 방식을 사용하여야 할 지에 대해 실습을 통해 알아보도록 하겠습니다.

***교차 검증(cross validation)*** 은 과적합 방지를 위해 데이터를 분리하는 방법입니다.

기존 실습 문제들에서 학습용 데이터와 테스트용 데이터로 분리하는 방법을 적용하였는데, 이번 실습에서는 테스트뿐만 아니라 검증용(Validation) 데이터를 활용하여 모델 결과를 평가해 보겠습니다.

일반적으로 교차 검증에서 가장 많이 사용되는 것이 바로 ***k-fold 교차 검증***입니다.

k-fold 교차 검증은 아래와 같이 학습용 데이터를 계속 변경하며 모델을 훈련시키는 방법입니다.

![image](https://cdn-api.elice.io/api-attachment/attachment/dbf627faf5b04330a880056cf642c98d/image.png)

k-fold 교차 검증은 다음과 같은 순서로 진행됩니다.

1. 데이터를 train, test 데이터로 분할
2. K를 설정하여 train 데이터 셋을 K개로 나눔
3. K개 중 한 개를 검증용, 나머지를 학습용으로 사용
4. K개 모델의 평균 성능 확인

------

***K-fold 교차 검증을 위한 사이킷런 함수/라이브러리***

- `kf=KFold(n_splits)`: K-fold 교차검증을 위한 KFold 객체 생성
- - n_splits : 분리할 데이터(fold) 개수
- `kf.split(X)`
- - `kf`를 통해 실제로 train과 test로 분리된 데이터의 인덱스를 반환합니다.
  - `X`: 분리하고자 하는 데이터

## ***실습***

1. 사이킷런에 존재하는 데이터를 불러오고, 불러온 데이터를 학습용, 테스트용 데이터로 분리하여 반환하는 `load_data()` 함수를 구현합니다.
2. - 사이킷런에 존재하는 boston 데이터를(X, y)의 형태로 불러옵니다.
   - 불러온 데이터를 학습용 데이터와 테스트용 데이터로 분리합니다.
   - ***학습용 데이터로 전체 데이터의 80%***를 사용하고, ***테스트용 데이터로 나머지 20%***를 사용합니다. 동일한 결과 확인을 위하여`random_state`를 ***100***으로 설정합니다.
3. K-fold 교차 검증을 통한 모델 학습 및 예측 수행을 진행하는 `kfold_regression()` 함수를 구현합니다.
4. - 전체 데이터를 ***5***개로 분리할 수 있도록 KFold 객체를 정의합니다.
   - 정의한 kFold 객체와 `.split()` 함수를 이용해 학습용 데이터 내에서 다시 학습용(Train) 데이터와 검증용(Validation)데이터를 나누고 각각 `X_train`, `X_val`, `y_train`, `y_val`에 저장합니다.
   - `train_idx`와 `val_idx`는 분리된 데이터들의 인덱스입니다.
   - 분리한 학습용 데이터로 모델을 학습시키고, 검증용 데이터로 모델을 평가하여 각 데이터에 대한 모델 평가 점수를 `score`변수에 저장합니다.
5. 실행 버튼을 눌러 검증 데이터에 대한 평균 평가 점수를 확인합니다.

------

## Tips!

- 각 fold에서 모델의 가중치를 동일하게 초기화 하기 위해서는 model을 정의하는 코드는 k-fold 교차 검증을 수행하는 for문 내에 위치해야 합니다.
- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.

###  

## 6. 릿지(Ridge), 라쏘(Lasso) 회귀

정규화 방법을 적용한 총 3가지(릿지, 라쏘, 엘라스틱 넷) 과적합 방지 모델 중 먼저 릿지(Ridge), 라쏘(Lasso) 회귀에 대해 알아보겠습니다.

사이킷런에 구현된 릿지 회귀와 라쏘 회귀를 이용하여 데이터를 학습시키고, βi들의 크기를 확인해 보겠습니다.



------


***사이킷런 데이터의 변수 이름을 불러오기 위한 방법***

- `load_boston().feature_names` : boston 데이터의 변수 이름을 반환합니다.

***릿지(Ridge), 라쏘(Lasso) 회귀를 위한 사이킷런 라이브러리/함수***

- `from sklearn.linear_model import Ridge` : 사이킷런에 저장된 릿지 회귀를 불러옵니다.
- `ridge_reg=Ridge(alpha=a)`: 릿지 회귀 모델 `ridge_reg`를 정의합니다.
- - `alpha`: 기본값은 1입니다.
  - alpha값이 클수록 ***더 강한 정규화***를 적용합니다.
- `ridge_reg.fit(X, y)`: `ridge_reg` 모델에 데이터 `X, y`를 학습시킵니다.
- `from sklearn.linear_model import Lasso` : 사이킷런에 저장된 라쏘 회귀를 불러옵니다.
- `lasso_reg=Lasso(alpha=a)`: 라쏘 회귀 모델 `lasso_reg`를 정의합니다.
- - `alpha`: 기본값은 1입니다.
  - alpha값이 클수록 더 강한 정규화를 적용합니다.
- `lasso_reg.fit(X, y)`: `lasso_reg` 모델에 데이터 `X, y`를 학습시킵니다.

## ***실습***

1. 데이터와 변수 이름을 불러오는 `load_data()` 함수를 구현합니다.
2. - 데이터의 변수 이름을 `feature_names` 에 저장합니다.
3. 릿지 회귀를 구현하고 데이터를 바탕으로 학습시킨 모델을 반환하는 `Ridge_regression()` 함수를 완성합니다.
4. - 사이킷런에 구현되어 있는 릿지(Ridge) 회귀 모델을 불러옵니다.
   - 파라미터 `alpha`를 ***10***으로 설정합니다.
   - 불러온 모델을 전체 데이터에 맞춰 학습시킵니다.
5. 라쏘 회귀를 구현하고 데이터를 바탕으로 학습시킨 모델을 반환하는 `Lasso_regression()` 함수를 완성합니다.
6. - 사이킷런에 구현되어 있는 라쏘(Lasso) 회귀 모델을 불러옵니다.
   - 파라미터 `alpha`를 ***10***으로 설정합니다.
   - 불러온 모델을 전체 데이터에 맞춰 학습시킵니다.
7. 실행 버튼을 눌러 릿지 회귀와 라쏘 회귀의 각 변수의 βi 값들을 확인하고, 각 회귀의 특징을 이해해봅니다.

- - 그래프를 통해 각 변수들의 βi의 크기를 살펴보고, ***라쏘 회귀와 릿지 회귀의 차이점을 생각해보세요***.

[실행 결과]

![image_output (6).png](https://cdn-api.elice.io/api-attachment/attachment/92173866806d4e729d9a1fe2d792248c/image_output%20%286%29.png)

![image_output (7).png](https://cdn-api.elice.io/api-attachment/attachment/5d7e5df50e3844808720f7c432f30fb3/image_output%20%287%29.png)

------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## 7. 엘라스틱넷(ElasticNet) 회귀

이전 실습을 통해 라쏘 회귀와 릿지 회귀에 대해 알아보았습니다. 이전 실습에서 그래프를 통해 확인한 바와 같이 라쏘(L1 정규화) 회귀는 너무 많은 βi를 0으로 만들고, 릿지(L2 정규화) 회귀는 βi를 완전히 0으로 만들지는 않기 때문에 여전히 모델의 복잡성이 높습니다.

따라서, 이 L1, L2 정규화를 동시에 사용하여 두 회귀의 장점을 가진 회귀인 ***엘라스틱넷 회귀가 등장***하게 된 것입니다.

엘라스틱넷 회귀는 L1 ,L2 정규화를 반영한 비율을 설정할 수 있기 때문에 결과를 확인해 가며 자유롭게 비율을 조정해 높은 예측 성능을 가진 모델을 만들어보세요.



------

***엘라스틱넷 회귀를 위한 사이킷런 함수/라이브러리***

- `from sklearn.linear_model import ElasticNet`: 엘라스틱넷 회귀를 불러옵니다.
- `ElasticNet_reg=ElasticNet(alpha, l1_ratio)`: ElasticNet모델 `ElasticNet_reg`을 정의합니다.
- - `alpha`: 클수록 더 강한 정규화를 적용합니다. 기본값은 1입니다.
  - `l1_ratio`: L1 정규화를 반영할 비율
- `ElasticNet_reg.fit(X, y)`: `ElasticNet_reg`에 데이터 `X, y`를 학습시킵니다.

## ***실습***

1. 데이터와 변수 이름을 불러오는 `load_data()` 함수를 구현합니다.
2. - 데이터의 변수 이름을 `feature_names` 에 저장합니다. [실습6]에서 구현한 함수를 그대로 이용할 수 있습니다.
3. 엘라스틱넷 회귀를 구현하고, 데이터를 바탕으로 학습시킨 모델을 반환하는`ElasticNet_regression()` 함수를 완성합니다.
4. - 사이킷런에 구현되어 있는 엘라스틱넷(ElasticNet) 회귀 모델을 불러옵니다.
   - 파라미터 `alpha`와 `l1_ratio`를 자유롭게 설정합니다.
   - 불러온 모델을 학습용 데이터에 맞춰 학습시킵니다.
5. 실행 버튼을 눌러 엘라스틱넷 회귀의 βi와 모델 평가 점수를 확인하고, `alpha,` `l1_ratio`값을 변경하며 결과를 확인합니다.
6. `alpha`, `l1_ratio`값을 변경하여 모델 평가 점수를 ***0.75 이상***으로 높입니다.



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## 8. 회귀 알고리즘 평가 지표- RSS

이번 실습에서는 회귀 알고리즘을 위한 다양한 평가 지표 중 가장 기본적인 ***Loss Function인 RSS(단순 오차제곱합)***을 직접 계산해 보고, 결과를 확인해 보겠습니다.

![image](https://cdn-api.elice.io/api-attachment/attachment/adb65e2042074ffa87c11e5b728d17c0/image.png)

## ***실습***

1. 회귀 알고리즘의 평가 지표 중 단순 오차 제곱합인 RSS(Residual Sum of Squares) 를 계산하여 반환하는 함수 `return_RSS()`를 완성합니다.

- - ***잔차제곱합(RSS)***: 예측값과 실제값의 차이의 제곱의 합

1. 정의한 함수들을 이용하여 `main()` 함수를 완성합니다.
2. - 생성한 데이터를 ***학습용 데이터와 테스트 데이터로 분리***하여 반환하는 함수를 호출합니다.
   - 학습용 데이터를 바탕으로 학습한 ***선형 회귀모델을 반환***하는 함수를 호출합니다.
   - 학습된 모델을 바탕으로 계산된 테스트 데이터의 예측값을 `predicted`에저장합니다.
   - 회귀 알고리즘을 평가하기 위한 RSS 값을 `RSS`에저장합니다.
3. 실행 버튼을 눌러 계산된 RSS값을 확인합니다.



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## **회귀 알고리즘 평가 지표- MSE, MAE**

이번 실습에서는 MSE와 MAE에 대해 알아보겠습니다.

MSE와 MAE는 사이킷런 라이브러리 안에 존재하여 쉽게 불러오기 만으로도 회귀 모델 평가 지표를 계산할 수 있습니다.

------

**MSE, MAE 평가 지표를 계산하기 위한 사이킷런 함수/라이브러리**

- `from sklearn.metrics import mean_absolute_error` : MAE 평가 지표를 계산해주는 `mean_absolute_error` 함수 불러오기
- `mean_absolute_error(y_test, y_pred)`: MAE 값 계산하기
- `from sklearn.metrics import mean_squared_error`: MSE 평가 지표를 계산해주는 `mean_squared_error` 함수 불러오기
- `mean_squared_error(y_test, y_pred)`: MSE 값 계산하기

## ***실습*** 

1. 모델 평가 지표인 MSE 값과 MAE 값을 반환하는 `main()` 함수를 순서에 맞게 완성합니다.

- - `MSE` 변수에는 해당 회귀 알고리즘의 MSE 값을, 변수 `MAE`에는 MAE 값을 저장합니다.

1. 실행 버튼을 눌러 계산된 MSE, MAE 값을 확인합니다.

- - 두 지표 값을 비교해보세요.



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## **회귀 알고리즘 평가 지표- R_squared**

이번 실습에서는 R2*R*2 에 대해 알아보겠습니다.

R2*R*2 또한 사이킷런 모듈 안에 존재하여 쉽게 불러오기 만으로도 회귀 모델 평가 지표를 계산할 수 있습니다.

특히 이전 실습들에서 살펴본 RSS, MSE, MAE 지표들은 “오차” 에 기반한 지표들이기 때문에 해당 값들이 작을수록 더 높은 성능의 모델을 의미하지만, R2*R*2 지표의 경우 데이터에 대한 모델의 설명력을 의미하기 때문에 1에 가까울수록 즉, 더 클수록 높은 성능의 모델임을 의미합니다.

------

**R2R2 평가 지표를 계산하기 위한 사이킷런 함수/라이브러리**

- `from sklearn.metrics import r2_score` : R2*R*2 평가 지표를 계산해주는 `r2_score` 함수 불러오기
- `r2_score(y_true, y_pred)` : R2*R*2 값 계산하기

## ***실습***

1. 모델 평가 지표인 R2 값을 반환하는 `main()` 함수를 순서에 맞게 완성합니다.

- - 생성한 데이터를 ***학습용 데이터와 테스트 데이터로 분리***하여 반환하는 함수를 호출합니다.
  - 학습용 데이터를 바탕으로 학습한 ***선형 회귀모델을 반환***하는 함수를 호출합니다.
  - 학습된 모델을 바탕으로 계산된 테스트 데이터의 예측값을 `predicted`에 저장합니다.
  - 회귀 알고리즘을 평가하기 위한 r2_score값을 `R_squared`에 저장합니다.

1. 실행 버튼을 눌러 오차와 다른 R2 값을 확인합니다.

[실행 결과]

![image_output (3).png](https://cdn-api.elice.io/api-attachment/attachment/6b2213c5af96408f91bdc0b7d2e81f8f/image_output%20%283%29.png)



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.